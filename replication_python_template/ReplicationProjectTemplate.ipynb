{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9459c8dc",
   "metadata": {},
   "source": [
    "---\n",
    "title: A template for Quantitative Finance Research Replication \n",
    "authors:\n",
    "  - name: First Author\n",
    "    email: youremail@illinois.edu\n",
    "    department: Financial Engineering\n",
    "    affiliation: University of Illinois\n",
    "  - name: Second Author (optional, continue for third, fourth, etc)\n",
    "    email: youremail@illinois.edu\n",
    "    department: Financial Engineering\n",
    "    affiliation: University of Illinois\n",
    "abstract: |\n",
    "  Enter the text of your abstract here. Consider your précis from the summary\n",
    "  and add 1-3 sentences about your conclusions.  6-10 sentences.\n",
    "keywords:\n",
    "  - trading strategies\n",
    "  - quantitative analysis\n",
    "  - machine learning\n",
    "  - these are optional and can be removed\n",
    "nocite: |\n",
    "  @PetersonReplication, @jabref, @Rmarkdown, @Peterson2015, @jupyterbook, @jupytext\n",
    "bibliography: references.bib\n",
    "bibtex_bibfiles: references.bib\n",
    "bibtex_default_style: 'alpha'\n",
    "copyright: Copyright CC-BY ©2023 \n",
    "output: rticles::arxiv_article \n",
    "sphinx:\n",
    "  config:\n",
    "    bibtex_reference_style: author_year\n",
    "    bibtex_bibfiles: \"your_reference_file.bib\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969f886-827e-4fbf-8a99-37f5062a2e0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "Risk parity (RP) portfolios have become a prominent alternative to traditional stock-heavy allocations such as 60/40. While RP often looks like a simple heuristic—“allocate more to safer assets and less to risky assets”—Leverage Aversion and Risk Parity (Asness, Frazzini, and Pedersen, 2012) provides a unifying theoretical explanation: when many investors are unwilling or unable to apply leverage, they tend to “reach for risk” by buying higher-beta assets to achieve higher expected returns. This demand pressure flattens the Security Market Line (SML), implying that low-risk assets can offer unusually strong risk-adjusted returns, whereas high-risk assets can be overpriced and deliver weaker risk-adjusted performance. In that world, a portfolio that overweights low-risk assets and scales its overall risk level via leverage is not merely an empirical curiosity—it is a natural outcome of equilibrium pricing under leverage frictions.\n",
    "\n",
    "This project replicates the paper’s core risk parity construction using a four-asset universe aligned with the original study—equities, intermediate/long duration government bonds, credit, and commodities—and extends the analysis to modern “retail feasibility” constraints over a long sample (1990–2025). Following the spirit of the paper, we implement a transparent and reproducible RP baseline based on inverse-volatility allocation with a three-year (36-month) rolling estimation window, and then apply volatility targeting so that RP can be compared fairly to conventional benchmarks at similar risk. We further study how real-world frictions—most importantly leverage caps and borrowing spreads—reshape the strategy’s realized performance. Finally, motivated by the paper’s discussion of deleveraging risk during stress periods, we evaluate a trend-filtered (“cash-reserve”) overlay designed to proactively reduce exposure when major asset sleeves fall below trend, providing an engineering-oriented response to tail-risk regimes such as 2008 and 2022.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253fa8ba-c2f6-4a5a-bbde-42d0de22d17d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Paper Summary\n",
    "\n",
    "<!-- Start with a single paragraph in précis form. -->\n",
    "<!-- See @PetersonReplication p. 1-2 for details. -->\n",
    "<!-- Complete this section with paragraphs describing each major point in the paper. -->\n",
    "<!-- The entire summary will be 4-10 paragraphs. -->\n",
    " This replication project critically examines the methodology and empirical results of Asness, Frazzini, and Pedersen (2012), which proposes a multi-asset class portfolio strategy, primarily leveraging the principle of Equal Risk Contribution (ERC), often referred to as Risk Parity. The source paper hypothesizes that by allocating capital such that each asset contributes an equal amount of volatility to the total portfolio risk, one can achieve superior risk-adjusted returns (Sharpe ratio) and greater portfolio stability compared to traditional market-cap or naive weighting schemes. Our primary objective is to reproduce the core ERC construction methodology and backtest results using modern data and robust Python engineering practices, followed by a rigorous analysis of the strategy's sensitivity, out-of-sample performance, and potential for extension.\n",
    "\n",
    "The paper's first major contribution lies in its selection of assets, typically including major global market indices, bonds, commodities, and credit instruments, acknowledging their low historical correlation. The paper details the data cleaning and processing required to achieve consistent return streams, particularly for assets like Treasury bonds and commodities where pricing involves specific models. The central theme is that portfolio benefits arise not just from diversification across assets, but also through diversification of risk contributions.\n",
    "\n",
    "The analytical core of the paper is the optimization model, which seeks to minimize the distance between the realized risk contribution of each asset and the target risk contribution (equal for all assets). The paper rigorously defines the risk contribution metric and outlines the numerical solution technique, emphasizing the need for robust covariance and volatility estimation, often using exponentially weighted moving averages or similar techniques to capture volatility clustering.\n",
    "\n",
    "The source paper presents compelling empirical evidence, claiming that the ERC strategy consistently delivers a higher Sharpe Ratio and lower maximum drawdown than a traditional 60/40 benchmark or a minimum variance portfolio across multiple market cycles. The performance claims are supported by metrics such as cumulative wealth curves, rolling Sharpe ratios, and leverage dynamics over the backtest period.\n",
    "\n",
    "Crucially, the paper addresses potential pitfalls by conducting sensitivity tests on key parameters, such as the lookback window for volatility estimation. This section of the paper attempts to demonstrate that the results are not merely a product of data mining or over-optimization, a necessary element for any credible systematic strategy.\n",
    "\n",
    "Finally, the paper proposes several extensions, such as incorporating alternative allocation rules (e.g., dynamic trend-following signals) or applying the ERC framework to different asset subsets. Furthermore, it addresses practical implementation issues, including the impact of transaction costs and taxes on the final realized returns, which is vital for real-world applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913b517-a81f-4a6a-812c-eb393724645a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hypothesis Overview\n",
    "\n",
    "<!-- Formally detail the paper's key hypotheses. -->\n",
    "<!-- See @PetersonReplication p. 2 for details. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d58fd8-cefe-4929-903f-47adc67a059a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Literature Review\n",
    "\n",
    "<!-- Write your literature review. See @PetersonReplication p. 2-4 for details. This -->\n",
    "<!-- section must include paragraphs at least for the 3-5 key references for the -->\n",
    "<!-- paper to be replicated, similar work, implementation references, more recent -->\n",
    "<!-- references where available, and any references with attempt to refute the -->\n",
    "<!-- hypotheses of the replicated work.  A full literature review may contain 20-50 -->\n",
    "<!-- references.  Not all will be covered in the same level of detail.  Important -->\n",
    "<!-- references probably warrant an entire paragraph, but similar work can probably -->\n",
    "<!-- be covered together in 1-2 paragraphs for multiple related works. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f147ab0-9445-43d0-bf4d-2cd35eb74aad",
   "metadata": {},
   "source": [
    "# Replication\n",
    "\n",
    "<!-- Now we move on to the actual replication.  The sections included here are all -->\n",
    "<!-- necessary, but the may not be sufficient.  Add additional sections and sub-sections -->\n",
    "<!-- as required to describe your work and make your analytical case. -->\n",
    "\n",
    "## Data\n",
    "\n",
    "<!-- Describe the approach that the replication is taking to Data. -->\n",
    "<!-- See @PetersonReplication p. 4-5 for details. -->\n",
    "<!-- Describe both the data used in the original paper, and the data you are using -->\n",
    "<!-- for replication.  For your replicated data, include detailed descriptions of -->\n",
    "<!-- obtaining, parsing, and cleaning the data to prepare it for use.  Describe data -->\n",
    "<!-- quality issues. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4712182-8b12-48a2-9b68-51bfb9f51b43",
   "metadata": {},
   "source": [
    "## Replication of Key Analytical Techniques\n",
    "\n",
    "<!-- Model the Key Analytical Techniques from the paper to be replicated. -->\n",
    "<!-- See @PetersonReplication p. 5-6 for details. -->\n",
    "<!-- This section will vary significantly based on the paper being replicated. -->\n",
    "<!-- Describe your process as you work, documenting the steps you are taking, -->\n",
    "<!-- referencing any libraries, websites, or third party code that you use as part of -->\n",
    "<!-- your replication, and the decree to which your replication agrees or disagrees -->\n",
    "<!-- with the source material. Be sure to include summary statistics used in the -->\n",
    "<!-- original paper, as well as any additional summary statistics that you feel are -->\n",
    "<!-- relevant for checking the quality of your replication. -->\n",
    "\n",
    "### Technique 1\n",
    "\n",
    "### Technique 2\n",
    "\n",
    "### Technique 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd5e9e-0bcf-4185-a870-f350f72e1c8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hypothesis Tests\n",
    "\n",
    "<!-- After replicating the initial work, it is time to evaluate the hypotheses of -->\n",
    "<!-- the replicated work. Those hypotheses were identified above, before you started -->\n",
    "<!-- replication. Describe, in detail, the statistical tests you perform to refute or -->\n",
    "<!-- validate the hypotheses in the replicated work.  This should go beyond any explicit -->\n",
    "<!-- tests performed in the original paper. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773469a3-b750-4119-af51-4b1f520a7a93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extended Analysis\n",
    "\n",
    "<!-- Extend the analysis with more (recent) data or additional asset classes, and/or -->\n",
    "<!-- replicate similar or extended techniques and compare them to the original paper's methods. -->\n",
    "<!-- See @PetersonReplication p. 6-7 for details. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e41c9-11fe-4e2f-b998-2018fbc5b47c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overfitting\n",
    "\n",
    "<!-- Analyze the likelihood that the original paper is overfit.  Include data -->\n",
    "<!-- considerations, experiment design, model assumptions, parameterization, and -->\n",
    "<!-- biases, out of sample results, etc.  Assess how changes to these affects -->\n",
    "<!-- results, and produce an opinion on whether and how the original work is overfit, -->\n",
    "<!-- as well as what might be doable to reduce the degree of overfitting, and whether -->\n",
    "<!-- the main results would hold if the level of overfitting were reduced. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb3da2-0ebd-4e91-b5e6-f5375d1c6188",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Future Work\n",
    "\n",
    "<!-- What additional work on this topic should be performed in the future, if this -->\n",
    "<!-- project were to be picked up again or continued? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef44382-b0ac-4239-962d-c8166e169225",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "<!-- Summarize the project and describe your conclusions.  This sections can -->\n",
    "<!-- range from 1-2 paragraphs to 1-2 pages. -->\n",
    "\n",
    "\\newpage \n",
    "\n",
    "![CC-BY](cc_by_88x31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9056c27-f3f6-4052-9c4b-5a98fe28886f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "```{bibliography}\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
